---
title: "Lec05_classnote_LS"
author: "shuna"
date: "1/28/2022"
output:
  html_document: default
  pdf_document: default
---
#### Central Limit Theorem 

##### Uniform Distribution
 
```{r}
mean(runif(100, 0,1))
```

```{r}
y = c()
for (i in c(1:100000)){
  x = runif(100,-10,10)
  y[i] = mean(x)
}
```

```{r}
hist(y)
```


###### Poisson Distribution 
```{r}
y_2 = c()
for (i in c(1:100000)){
  x = rpois(100,10)
  y_2[i] = mean(x)
}
hist(y_2)
```


##### Binomial Distribution

```{r}
y_3 = c()
for (i in c(1:100000)){
  x = rbinom(100,10,0.5)
  y_3[i] = mean(x)
}
hist(y_3)
```

##### Genetics 

Sum of the multiple independent variables leads to normal distribution 
regression to the mean 


##### Assumption 

1. assuming normality is critical for **parametric hypothesis test** 
2. we use these test even when the distribution is non-normal - as long as the sample size is large enough 

###### Galton's box 

The sum of the probability of a ball going left or right eventually converges to a normal distribution. 


#### Regression to the Mean 


If two very two people have kids, the kids tend to be taller than average, but not as tall as the parents (same applies to intelligence). 

The tall parents tend to have children that are closer to the mean of the population. 


in any series of random events an extraordinary event is more likely to be followed, due to chance, by any more ordinary one. 

the more extreme a variable is upon its first measurement, the more likely it is to be closer to the average the second time it is measured. 

#### Confidence Interval 

```{r}
gen_1 = c()

for (i in c(1:10)){
  x = mean(rnorm(10,70,3))
  gen_1[i] = x
}
hist(gen_1)
```
```{r}
gen_2 = c()

for (i in c(1:10)){
  x = mean(rnorm(50,70,3))
  gen_2[i] = x
}
hist(gen_2)
```


```{r}
gen_3 = c()

for (i in c(1:10)){
  x = mean(rnorm(10,70,10))
  gen_3[i] = x
}
hist(gen_3)
```

```{r}
gen_4 = c()

for (i in c(1:10)){
  x = mean(rnorm(10,70,10))
  gen_4[i] = x
}
hist(gen_4)
```


Our estimate of a population from a sample depends on 
- variation --> $\sigma$
- sample size --> $n$

#### SEM

$$
SEM = \frac{s}{\sqrt{n}}
$$

Qualifies how precisely you know the population mean 
- standard error of the mean is a measure of the dispersion of sample means around the population mean.


Confidence intervals depend on...

- the sample mean 
- standard deviation 
- sample size 
- degree of confidence 


##### bootstrapping 

```{r}
boot = c()
for (i in 1:10000){
  x = sample(rnorm(50,78,5), 
             size = 50,
             replace = TRUE) 
  boot[i] = mean(x)
}
hist(boot)
abline(v=c(quantile(boot, 0.025),
           quantile(boot, 0.975)), 
       col=c("blue", "blue"),
       lty=c(1, 1), 
       lwd=c(1, 1))
```

##### when SD is unknown 

$$
\mu = x + t * \left(\frac{s}{\sqrt{n}}\right) \\

\mu = x - t * \left(\frac{s}{\sqrt{n}}\right)
$$
- t-distribution : the t-distribution is the distribution of the location of the sample mean relative to the population mean 

how do we calculate the t-distribution? 

$$
\left(r,q,p,d\right) t
$$
```{r}
lb = qt(0.025, 49)*(sd(boot)/sqrt(length(boot)))
ub = qt(0.975, 49)*(sd(boot)/sqrt(length(boot)))

mean(boot) + ub
mean(boot) + lb
```


#### Degree of freedom 

- number of values that are free to vary in the data 
- why is degree of freedom n-1? 

lets say we pick three numbers that have a mean of 10 
- once you pick two numbers, because the mean is 10, we already know the third number. 
- so the degree of freedom has to be 2 

###### t-distribution
$$
t = \frac{m - \mu}{s / \sqrt{n} }
$$
m = mean of the sample 
$\mu$ = mean of the population


```{r}

t = c()
for (i in 1:10000){
  x = sample(rnorm(50, 78,5), size = 50, replace = TRUE)
  m = mean(x)
  s = sd(x)
  t[i] = (m - 78) / (s/sqrt(50))
}

hist(t,breaks = 'FD')
```

plotting the sample mean on the population mean, basically 